{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARISHREDDYCHILUMULA/ML_Projects/blob/main/Detection_of_Horses_and_Persons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M0E2zlTZI7g"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/https-deeplearning-ai/tensorflow-1-public/blob/master/C1/W4/ungraded_labs/C1_W4_Lab_1_image_generator_no_validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-74XLLwqPlcw"
      },
      "source": [
        "# Classification of Humans and Horses using Convolutional Neural Networks\n",
        "\n",
        "I have taken dataset from [Tensorflow.org](https://www.tensorflow.org/datasets/catalog/horses_or_humans) . This contains over a thousand images of horses and humans with varying poses and filesizes. [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) class is used to prepare this dataset so it can be fed to a convolutional neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYFguQkJvpV3"
      },
      "source": [
        "Below code is to download the compressed dataset `horse-or-human.zip` which contains thousands of images of Horses and Humans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXZT2UsyIVe_"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9brUxyTpYZHy"
      },
      "source": [
        "Unzipping the archive using the [zipfile](https://docs.python.org/3/library/zipfile.html) module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLy3pthUS0D2"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip the dataset\n",
        "local_zip = './horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./horse-or-human')\n",
        "zip_ref.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-qUPyfO7Qr8"
      },
      "source": [
        "The contents of the .zip are extracted to the base directory `./horse-or-human`, which in turn each contain `horses` and `humans` subdirectories.\n",
        "\n",
        "In short: The training set is the data that is used to tell the neural network model that 'this is what a horse looks like' and 'this is what a human looks like'.\n",
        "\n",
        "One thing to pay attention to in this sample: We do not explicitly label the images as horses or humans. You will use the ImageDataGenerator API instead -- and this is coded to automatically label images according to the directory names and structure. So, for example, you will have a 'training' directory containing a 'horses' directory and a 'humans' one. `ImageDataGenerator` will label the images appropriately for you, reducing a coding step. \n",
        "\n",
        "You can now define each of these directories:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The os.path.join() function in Python is used to join one or more path components intelligently. It is a platform-independent method of joining directory and file names in a way that is appropriate for the operating system. This means that it can be used on any operating system such as Windows, Linux, and macOS.\n",
        "\n",
        "The function takes one or more path components as arguments and joins them using the appropriate separator for the operating system. For example, on Windows, the separator is the backslash (\\), while on Unix-based systems, it is the forward slash (/)."
      ],
      "metadata": {
        "id": "r8Cc0XrfJYzA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR_M9nWN-K8B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('./horse-or-human/humans')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuBYtA_Zd8_T"
      },
      "source": [
        "Now see what the filenames look like in the `horses` and `humans` training directories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PIP1rkmeAYS"
      },
      "outputs": [],
      "source": [
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "print(train_horse_names[:10])\n",
        "\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "print(train_human_names[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlqN5KbafhLI"
      },
      "source": [
        "You can also find out the total number of horse and human images in the directories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4XHh2xSfgie"
      },
      "outputs": [],
      "source": [
        "print('total training horse images:', len(os.listdir(train_horse_dir)))\n",
        "print('total training human images:', len(os.listdir(train_human_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WZABE9eX-8"
      },
      "source": [
        "Now take a look at a few pictures to get a better sense of what they look like. First, configure the `matplotlib` parameters:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The %matplotlib inline magic command is used to display Matplotlib plots in the output cells of the Jupyter notebook or IPython console. This command tells the notebook to render any plots generated by Matplotlib directly in the output cells, rather than in separate windows or external files.\n",
        "\n",
        "The import matplotlib.pyplot as plt statement imports the Pyplot module from the Matplotlib library, which provides a simple interface for creating plots and visualizations. \n",
        "\n",
        "The import matplotlib.image as mpimg statement imports the mpimg module from the Matplotlib library, which provides a way to load and display image files within a plot."
      ],
      "metadata": {
        "id": "sCykBf0PMsHl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2_Q0-_5UAv-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvHzGCxXkqp"
      },
      "source": [
        "Now, display a batch of 8 horse and 8 human pictures. You can rerun the cell to see a fresh batch each time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpr8GxjOU8in"
      },
      "outputs": [],
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
        "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
        "next_human_pix = [os.path.join(train_human_dir, fname) \n",
        "                for fname in train_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "## Building a Small Model from Scratch\n",
        "\n",
        "Now you can define the model architecture that you will train.\n",
        "\n",
        "Step 1 will be to import tensorflow."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network"
      ],
      "metadata": {
        "id": "i6iVVw5eNotb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Network is a type of neural network commonly used for image classification, object detection, and other computer vision tasks.\n",
        "\n",
        "CNNs are designed to automatically and adaptively learn spatial hierarchies of features from input data, such as images. They consist of multiple layers of interconnected neurons that learn to recognize and extract increasingly complex features from the input image. These layers typically include convolutional layers, pooling layers, and fully connected layers.\n",
        "\n",
        "Convolutional layers are the key component of CNNs, and they consist of a set of filters or kernels that are convolved with the input image to produce a set of feature maps. The filters are learned during training, and they are designed to recognize specific patterns or features in the input image.\n",
        "\n",
        "Pooling layers are used to downsample the feature maps produced by the convolutional layers, which reduces the spatial dimensionality of the data and helps to reduce overfitting. There are several types of pooling layers, such as max pooling and average pooling.\n",
        "\n",
        "Fully connected layers are used at the end of the CNN to classify the input image into one of several categories. These layers are similar to the ones used in traditional neural networks, and they consist of multiple neurons that are fully connected to the output of the previous layer."
      ],
      "metadata": {
        "id": "67H4yosPNxES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorFlow**\n",
        "\n",
        "TensorFlow is an open-source machine learning framework developed by Google Brain team for building and training various types of machine learning models, including neural networks. It is one of the most widely used machine learning frameworks, along with PyTorch and Keras.\n",
        "\n",
        "TensorFlow provides a flexible platform for building and deploying machine learning models at scale. It allows developers to build models using a high-level API (such as Keras) or by writing low-level code using its core APIs. TensorFlow also provides tools for data preprocessing, model training, and model evaluation, as well as support for distributed training across multiple devices and machines.\n",
        "\n",
        "One of the key features of TensorFlow is its ability to build and train deep neural networks, including convolutional neural networks (CNNs) and recurrent neural networks (RNNs). TensorFlow also includes a variety of pre-built models, such as object detection models, image classification models, and natural language processing models, which can be easily fine-tuned for specific use cases."
      ],
      "metadata": {
        "id": "ZmAThjAYN5j1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvfZg3LQbD-5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conv2D**\n",
        "\n",
        "Conv2D layer is used to apply convolutional operations on 2D image data. It consists of a set of filters that are convolved with the input image to produce a set of feature maps. Each filter learns to detect a specific pattern or feature in the input image. During training, the weights of these filters are learned through backpropagation."
      ],
      "metadata": {
        "id": "T_vXvW3mO16-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MaxPooling**\n",
        "\n",
        "MaxPooling2D layer is used to downsample the feature maps produced by the Conv2D layer. It reduces the spatial size of the feature maps while retaining the most important features. MaxPooling2D operates on each feature map independently and applies a max operation to each non-overlapping window of a specified size."
      ],
      "metadata": {
        "id": "sHH-hWlEO5bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flatten**\n",
        "\n",
        " The Flatten layer reshapes this tensor into a 1D vector, which can then be fed into the fully connected layers for classification or regression."
      ],
      "metadata": {
        "id": "1FMqifLXPsLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dense Layer**\n",
        "\n",
        "Dense layer is a fully connected layer in which every neuron in the layer is connected to every neuron in the previous layer. This means that each neuron in the Dense layer receives input from all the neurons in the previous layer and computes a weighted sum of the inputs, followed by an activation function."
      ],
      "metadata": {
        "id": "WYDLxOAFQruM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Activation Function**\n",
        "\n",
        "An activation function is a mathematical function that is applied to the output of a neuron in a neural network. It introduces non-linearity to the output of the neuron, allowing the neural network to learn complex patterns in the input data.\n",
        "\n",
        "**ReLU** (Rectified Linear Unit): The ReLU function returns 0 for negative inputs and the input value for positive inputs. It is the most commonly used activation function in neural networks due to its simplicity and effectiveness in preventing the vanishing gradient problem.\n",
        "\n",
        "**Sigmoid**: The sigmoid function maps any real-valued number to a value between 0 and 1. It is commonly used in the output layer of a binary classification problem."
      ],
      "metadata": {
        "id": "ZUogCfZiPQJI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PixZ2s5QbYQ3"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # The input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    # This is the first convolution with 16 filters of size 3*3\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution with 32 filters of size 3*3\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution with 64 filters of size 3*3\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution with 64 filters of size 3*3\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution with 64 filters of size 3*3\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9EaFDP5srBa"
      },
      "source": [
        "You can review the network architecture and the output shapes with `model.summary()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZKj8392nbgP"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEkKSpZlvJXA"
      },
      "source": [
        "Train the model with the `binary_crossentropy` loss because it's a binary classification problem, and the final activation is a sigmoid. \n",
        "\n",
        "In this case, using the `RMSprop optimization algorithm` is preferable to stochastic gradient descent, because RMSprop automates learning-rate tuning for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DHWhFP_uhq3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.001),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn9m9D3UimHM"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Next step is to set up the data generators that will read pictures in the source folders, convert them to `float32` tensors, and feed them (with their labels) to the model. You'll have one generator for the training images and one for the validation images. These generators will yield batches of images of size 300x300 and their labels (binary).\n",
        "\n",
        "Data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network (i.e. It is uncommon to feed raw pixels into a ConvNet.) In this case, we'll will preprocess the images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n",
        "\n",
        "In Keras, this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Data Generator**\n",
        "\n",
        "The ImageDataGenerator class in TensorFlow is a powerful tool for creating data augmentation pipelines for image data. It allows you to generate augmented images on-the-fly during model training, which can help improve the accuracy and robustness of your models by exposing them to a wider range of input variations."
      ],
      "metadata": {
        "id": "qr49sqEvSJ4a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClebU9NJg99G"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        './horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(300, 300),  # All images will be resized to 300x300\n",
        "        batch_size=128,\n",
        "        class_mode='binary') #class_mode specifies the type of label array that is returned by the generator.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mu3Jdwkjwax4"
      },
      "source": [
        "### Training\n",
        "\n",
        "Let's train the model for 15 epochs.\n",
        "\n",
        "The `loss` and `accuracy` are great indicators of progress in training. `loss` measures the current model prediction against the known labels, calculating the result. `accuracy`, on the other hand, is the portion of correct guesses. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Epoch** refers to one complete pass through the entire training dataset during model training.\n",
        "**verbose** parameter is used in the model.fit() method to control the amount of logging output that is displayed during model training.\n",
        "verbose can take one of three possible values:\n",
        "\n",
        "0: No logging output is displayed during training.\n",
        "\n",
        "1: A progress bar is displayed during training that shows the number of epochs completed, the time elapsed, and the training and validation loss and accuracy.\n",
        "\n",
        "2: A progress bar is not displayed, but the training and validation loss and accuracy are printed to the console after each epoch."
      ],
      "metadata": {
        "id": "bJqlyRBkTDpM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb1_lgobv81m"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vSHzPR2ghH"
      },
      "source": [
        "### Model Prediction\n",
        "\n",
        "Now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, upload them, and run them through the model, giving an indication of whether the object is a horse or a human.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoWp43WxJDNT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = load_img(path, target_size=(300, 300))\n",
        "  x = img_to_array(img)\n",
        "  x /= 255\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "    \n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "\n",
        "To terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "651IgjLyo-Jx"
      },
      "outputs": [],
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9kApw_7zJ0X7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}